{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to access functions from other locations\n",
    "import sys\n",
    "sys.path.append('/data/ad181/RemoteDir/rl_robust_owc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "\n",
    "import gym\n",
    "from stable_baselines3.ppo import PPO, MlpPolicy\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv, DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import CallbackList\n",
    "from utils.custom_eval_callback import CustomEvalCallback\n",
    "from utils.env_wrappers import StateCoarse, BufferWrapper\n",
    "from typing import Callable\n",
    "from utils.plot_functions import plot_learning\n",
    "\n",
    "from model.ressim import Grid\n",
    "from ressim_env import ResSimEnv_v0, ResSimEnv_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=1\n",
    "case='case_1_ppo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('./data', exist_ok=True)\n",
    "os.makedirs('./data/'+case, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../envs_params/env_data/env_train.pkl', 'rb') as input:\n",
    "    env_train = pickle.load(input)\n",
    "    \n",
    "with open('../envs_params/env_data/env_eval.pkl', 'rb') as input:\n",
    "    env_eval = pickle.load(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env wrapper parameters\n",
    "x_coords, y_coords = env_train.p_x, env_train.p_y\n",
    "\n",
    "def env_wrappers(env, x_coords, y_coords):\n",
    "    env_ = deepcopy(env)\n",
    "    env_ = StateCoarse(env_, x_coords, y_coords, include_well_pr=True)\n",
    "    return env_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(env, rank: int, seed: int) -> Callable:\n",
    "    \"\"\"\n",
    "    Utility function for multiprocessed env.\n",
    "    \n",
    "    :param env_id: (str) the environment ID\n",
    "    :param num_env: (int) the number of environment you wish to have in subprocesses\n",
    "    :param seed: (int) the inital seed for RNG\n",
    "    :param rank: (int) index of the subprocess\n",
    "    :return: (Callable)\n",
    "    \"\"\"\n",
    "    def _init() -> gym.Env:\n",
    "        env_ = env\n",
    "        env_.seed(seed + rank)\n",
    "        return env_\n",
    "    return _init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(-100000.0, 100000.0, (3721,), float64)\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "(93,)\n",
      "(93,)\n",
      "(93,)\n",
      "(93,)\n",
      "(93,)\n"
     ]
    }
   ],
   "source": [
    "# test run\n",
    "# env = StateCoarse(env_train, x_coords, y_coords, include_well_pr=True)\n",
    "# print(env_train.observation_space)\n",
    "# base_action = np.ones(env.action_space.shape[0])\n",
    "\n",
    "# state, done = env.reset(), False\n",
    "# print(state)\n",
    "# while not done:\n",
    "#     state, reward, done, info = env.step(base_action)\n",
    "#     print(state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed 1\n",
      "Box(-100000.0, 100000.0, (93,), float64)\n",
      "seed 1: model definition ..\n",
      "Using cuda device\n",
      "seed 1: learning ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/Paper_1_codes_revised/utils/custom_eval_callback.py:97: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7f61ac362ac8> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f6121bc1668>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n",
      "/data/ad181/RemoteDir/Paper_1_codes_revised/utils/custom_eval_callback.py:97: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7f61ac362ac8> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f6121bc16a0>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 97   |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 32   |\n",
      "|    total_timesteps | 3200 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=6400, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6400, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.597       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 49          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 130         |\n",
      "|    total_timesteps      | 6400        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008551882 |\n",
      "|    clip_fraction        | 0.191       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | -0.398      |\n",
      "|    learning_rate        | 1e-06       |\n",
      "|    loss                 | 0.071       |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0185     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.132       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 51          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 185         |\n",
      "|    total_timesteps      | 9600        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017526306 |\n",
      "|    clip_fraction        | 0.211       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | -1.15       |\n",
      "|    learning_rate        | 1e-06       |\n",
      "|    loss                 | 0.0997      |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0178     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0723      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=12800, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=12800, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.596       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 45          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 280         |\n",
      "|    total_timesteps      | 12800       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020338265 |\n",
      "|    clip_fraction        | 0.214       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | -1.2        |\n",
      "|    learning_rate        | 1e-06       |\n",
      "|    loss                 | 0.0946      |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0171     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0536      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 47          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 337         |\n",
      "|    total_timesteps      | 16000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020993676 |\n",
      "|    clip_fraction        | 0.208       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | 91.8        |\n",
      "|    explained_variance   | -0.82       |\n",
      "|    learning_rate        | 1e-06       |\n",
      "|    loss                 | 0.087       |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0172     |\n",
      "|    std                  | 0.055       |\n",
      "|    value_loss           | 0.0449      |\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for seed in range(1,4):\n",
    "    if True:\n",
    "        print(f'seed {seed}')\n",
    "        log_dir = './data/'+case+'/seed_'+str(seed)\n",
    "        os.makedirs(log_dir, exist_ok=True)\n",
    "        num_cpu = 64\n",
    "        env_train.seed(seed)\n",
    "        env_eval.seed(seed)\n",
    "        env_train_ = env_wrappers(env_train, x_coords, y_coords)\n",
    "        env_eval_ = env_wrappers(env_eval, x_coords, y_coords)\n",
    "        train_callback = CustomEvalCallback( env_train_, \n",
    "                                            best_model_save_path=None, \n",
    "                                            n_eval_episodes=1,\n",
    "                                            log_path=str(log_dir)+'/results_train', \n",
    "                                            eval_freq=100)\n",
    "        callback_list = [train_callback]\n",
    "        eval_callback = CustomEvalCallback( env_eval_, \n",
    "                                           best_model_save_path=str(log_dir)+'/best_model_eval', \n",
    "                                           n_eval_episodes=1,\n",
    "                                           log_path=str(log_dir)+'/results_eval', \n",
    "                                           eval_freq=100)\n",
    "        callback_list.append(eval_callback)\n",
    "        callback = CallbackList(callback_list)\n",
    "        env = SubprocVecEnv([make_env(env_train_, i, seed) for i in range(num_cpu)])\n",
    "        print(env.observation_space)\n",
    "#     env = VecMonitor(env, filename=log_dir)\n",
    "        print(f'seed {seed}: model definition ..')\n",
    "        model = PPO(policy=MlpPolicy,\n",
    "            env=env,\n",
    "            learning_rate = 1e-6,\n",
    "            n_steps = 50,\n",
    "            batch_size = 16,\n",
    "            n_epochs = 20,\n",
    "            gamma = 0.99,\n",
    "            gae_lambda = 0.95,\n",
    "            clip_range = 0.1,\n",
    "            clip_range_vf = None,\n",
    "            ent_coef = 0.001,\n",
    "            vf_coef = 0.5,\n",
    "            max_grad_norm = 0.5,\n",
    "            use_sde= False,\n",
    "            create_eval_env= False,\n",
    "            policy_kwargs = dict(net_arch=[150,100,80], log_std_init=-2.9),\n",
    "            verbose = 1,\n",
    "            target_kl = 0.05,\n",
    "            seed = seed,\n",
    "            device = \"auto\")\n",
    "        print(f'seed {seed}: learning ..')\n",
    "        model.learn(total_timesteps=300000, callback=callback)\n",
    "        model.save(log_dir+'/PPO')\n",
    "        del model\n",
    "        fig = plot_learning(log_dir, case='train')\n",
    "        fig.savefig(log_dir+'/learn_train.png')\n",
    "        fig = plot_learning(log_dir, case='eval')\n",
    "        fig.savefig(log_dir+'/learn_eval.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
