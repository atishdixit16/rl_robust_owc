{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to access functions from other locations\n",
    "import sys\n",
    "sys.path.append('/data/ad181/RemoteDir/k_variability_in_ressim_env/SPE10_like_envs/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gym\n",
    "import torch as th\n",
    "from stable_baselines3.a2c import A2C, MlpPolicy\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv, DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import CallbackList\n",
    "from utils.custom_eval_callback import CustomEvalCallback\n",
    "from typing import Callable\n",
    "from utils.env_wrappers import StateCoarse\n",
    "\n",
    "from utils.plot_functions import plot_learning\n",
    "\n",
    "from model.ressim import Grid\n",
    "from ressim_env import ResSimEnv_v0, ResSimEnv_v1, ResSimEnv_v2\n",
    "from k_distributions.generate_constr_k import generate_cond_\n",
    "from utils.env_wrappers import StepReset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=1\n",
    "case='1ph_v0_state_wells_sp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('./data', exist_ok=True)\n",
    "os.makedirs('./data/'+case, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_ = '1ph_v0'\n",
    "with open('../envs_params/env_data_v1/env_'+case_+'_train_cluster.pkl', 'rb') as input:\n",
    "    env_train = pickle.load(input)\n",
    "\n",
    "rl_indices = [24,5,14,1,6,12,0,25,22]\n",
    "with open('../envs_params/env_data_v1/env_'+case_+'_eval_cluster.pkl', 'rb') as input:\n",
    "    env_eval = pickle.load(input)\n",
    "k_list_rl = env_eval.k_list[rl_indices]\n",
    "env_eval.set_k(k_list_rl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env wrapper to reduce state space\n",
    "x_coords, y_coords = env_train.p_x, env_train.p_y\n",
    "\n",
    "def env_wrappers(env, x_coords, y_coords):\n",
    "    env = StateCoarse(env, x_coords, y_coords, include_well_pr=True)\n",
    "    env = StepReset(env)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(env, rank: int, seed: int = 0) -> Callable:\n",
    "    \"\"\"\n",
    "    Utility function for multiprocessed env.\n",
    "    \n",
    "    :param env_id: (str) the environment ID\n",
    "    :param num_env: (int) the number of environment you wish to have in subprocesses\n",
    "    :param seed: (int) the inital seed for RNG\n",
    "    :param rank: (int) index of the subprocess\n",
    "    :return: (Callable)\n",
    "    \"\"\"\n",
    "    def _init() -> gym.Env:\n",
    "        env_ = env\n",
    "        env_.seed(seed + rank)\n",
    "        return env_\n",
    "    return _init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed 1\n",
      "Box(-100000.0, 100000.0, (9,), float64)\n",
      "seed 1: model definition ..\n",
      "Using cuda device\n",
      "seed 1: learning ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/k_variability_in_ressim_env/SPE10_like_envs/utils/custom_eval_callback.py:97: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7f68f0503710> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f68f05036a0>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n",
      "/data/ad181/RemoteDir/k_variability_in_ressim_env/SPE10_like_envs/utils/custom_eval_callback.py:97: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7f68f0503710> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f68f05036d8>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 141      |\n",
      "|    iterations         | 10       |\n",
      "|    time_elapsed       | 11       |\n",
      "|    total_timesteps    | 1600     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.92     |\n",
      "|    explained_variance | -10.9    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 9        |\n",
      "|    policy_loss        | -0.938   |\n",
      "|    std                | 0.055    |\n",
      "|    value_loss         | 0.0664   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3200, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3200, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4        |\n",
      "|    mean_reward        | 0.62     |\n",
      "| time/                 |          |\n",
      "|    fps                | 105      |\n",
      "|    iterations         | 20       |\n",
      "|    time_elapsed       | 30       |\n",
      "|    total_timesteps    | 3200     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.92     |\n",
      "|    explained_variance | -17.5    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 19       |\n",
      "|    policy_loss        | -0.924   |\n",
      "|    std                | 0.0551   |\n",
      "|    value_loss         | 0.073    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 116      |\n",
      "|    iterations         | 30       |\n",
      "|    time_elapsed       | 41       |\n",
      "|    total_timesteps    | 4800     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.92     |\n",
      "|    explained_variance | -10.2    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 29       |\n",
      "|    policy_loss        | -0.639   |\n",
      "|    std                | 0.055    |\n",
      "|    value_loss         | 0.0465   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6400, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6400, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4        |\n",
      "|    mean_reward        | 0.625    |\n",
      "| time/                 |          |\n",
      "|    fps                | 106      |\n",
      "|    iterations         | 40       |\n",
      "|    time_elapsed       | 59       |\n",
      "|    total_timesteps    | 6400     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.93     |\n",
      "|    explained_variance | -8.94    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 39       |\n",
      "|    policy_loss        | -0.698   |\n",
      "|    std                | 0.055    |\n",
      "|    value_loss         | 0.0573   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 113      |\n",
      "|    iterations         | 50       |\n",
      "|    time_elapsed       | 70       |\n",
      "|    total_timesteps    | 8000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.92     |\n",
      "|    explained_variance | -4.34    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 49       |\n",
      "|    policy_loss        | -0.5     |\n",
      "|    std                | 0.055    |\n",
      "|    value_loss         | 0.0348   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9600, episode_reward=0.60 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=9600, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4        |\n",
      "|    mean_reward        | 0.622    |\n",
      "| time/                 |          |\n",
      "|    fps                | 107      |\n",
      "|    iterations         | 60       |\n",
      "|    time_elapsed       | 89       |\n",
      "|    total_timesteps    | 9600     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.93     |\n",
      "|    explained_variance | -5.93    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 59       |\n",
      "|    policy_loss        | -0.585   |\n",
      "|    std                | 0.055    |\n",
      "|    value_loss         | 0.0471   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 111      |\n",
      "|    iterations         | 70       |\n",
      "|    time_elapsed       | 100      |\n",
      "|    total_timesteps    | 11200    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.93     |\n",
      "|    explained_variance | -2.8     |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 69       |\n",
      "|    policy_loss        | -0.393   |\n",
      "|    std                | 0.055    |\n",
      "|    value_loss         | 0.0292   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=12800, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=12800, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4        |\n",
      "|    mean_reward        | 0.629    |\n",
      "| time/                 |          |\n",
      "|    fps                | 107      |\n",
      "|    iterations         | 80       |\n",
      "|    time_elapsed       | 118      |\n",
      "|    total_timesteps    | 12800    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.92     |\n",
      "|    explained_variance | -3.51    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 79       |\n",
      "|    policy_loss        | -0.471   |\n",
      "|    std                | 0.055    |\n",
      "|    value_loss         | 0.0368   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 110      |\n",
      "|    iterations         | 90       |\n",
      "|    time_elapsed       | 129      |\n",
      "|    total_timesteps    | 14400    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.93     |\n",
      "|    explained_variance | -1.81    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 89       |\n",
      "|    policy_loss        | -0.331   |\n",
      "|    std                | 0.055    |\n",
      "|    value_loss         | 0.0253   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=16000, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4        |\n",
      "|    mean_reward        | 0.634    |\n",
      "| time/                 |          |\n",
      "|    fps                | 107      |\n",
      "|    iterations         | 100      |\n",
      "|    time_elapsed       | 148      |\n",
      "|    total_timesteps    | 16000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.93     |\n",
      "|    explained_variance | -1.94    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 99       |\n",
      "|    policy_loss        | -0.432   |\n",
      "|    std                | 0.055    |\n",
      "|    value_loss         | 0.0291   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 110      |\n",
      "|    iterations         | 110      |\n",
      "|    time_elapsed       | 159      |\n",
      "|    total_timesteps    | 17600    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.93     |\n",
      "|    explained_variance | -0.354   |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 109      |\n",
      "|    policy_loss        | -0.282   |\n",
      "|    std                | 0.055    |\n",
      "|    value_loss         | 0.0187   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=19200, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=19200, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4        |\n",
      "|    mean_reward        | 0.636    |\n",
      "| time/                 |          |\n",
      "|    fps                | 107      |\n",
      "|    iterations         | 120      |\n",
      "|    time_elapsed       | 177      |\n",
      "|    total_timesteps    | 19200    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.93     |\n",
      "|    explained_variance | -0.575   |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 119      |\n",
      "|    policy_loss        | -0.325   |\n",
      "|    std                | 0.055    |\n",
      "|    value_loss         | 0.0213   |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 110      |\n",
      "|    iterations         | 130      |\n",
      "|    time_elapsed       | 188      |\n",
      "|    total_timesteps    | 20800    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.93     |\n",
      "|    explained_variance | -0.133   |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 129      |\n",
      "|    policy_loss        | -0.204   |\n",
      "|    std                | 0.055    |\n",
      "|    value_loss         | 0.0168   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=22400, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=22400, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "New best mean reward!\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4        |\n",
      "|    mean_reward        | 0.637    |\n",
      "| time/                 |          |\n",
      "|    fps                | 107      |\n",
      "|    iterations         | 140      |\n",
      "|    time_elapsed       | 207      |\n",
      "|    total_timesteps    | 22400    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.93     |\n",
      "|    explained_variance | -0.154   |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 139      |\n",
      "|    policy_loss        | -0.141   |\n",
      "|    std                | 0.055    |\n",
      "|    value_loss         | 0.017    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 109      |\n",
      "|    iterations         | 150      |\n",
      "|    time_elapsed       | 218      |\n",
      "|    total_timesteps    | 24000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.93     |\n",
      "|    explained_variance | 0.149    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 149      |\n",
      "|    policy_loss        | -0.2     |\n",
      "|    std                | 0.055    |\n",
      "|    value_loss         | 0.0153   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=25600, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "Eval num_timesteps=25600, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4        |\n",
      "|    mean_reward        | 0.635    |\n",
      "| time/                 |          |\n",
      "|    fps                | 107      |\n",
      "|    iterations         | 160      |\n",
      "|    time_elapsed       | 237      |\n",
      "|    total_timesteps    | 25600    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.93     |\n",
      "|    explained_variance | 0.14     |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 159      |\n",
      "|    policy_loss        | -0.159   |\n",
      "|    std                | 0.055    |\n",
      "|    value_loss         | 0.014    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 109      |\n",
      "|    iterations         | 170      |\n",
      "|    time_elapsed       | 247      |\n",
      "|    total_timesteps    | 27200    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.93     |\n",
      "|    explained_variance | 0.188    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 169      |\n",
      "|    policy_loss        | -0.182   |\n",
      "|    std                | 0.055    |\n",
      "|    value_loss         | 0.015    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=28800, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "Eval num_timesteps=28800, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4        |\n",
      "|    mean_reward        | 0.635    |\n",
      "| time/                 |          |\n",
      "|    fps                | 108      |\n",
      "|    iterations         | 180      |\n",
      "|    time_elapsed       | 266      |\n",
      "|    total_timesteps    | 28800    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.93     |\n",
      "|    explained_variance | 0.174    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 179      |\n",
      "|    policy_loss        | -0.131   |\n",
      "|    std                | 0.055    |\n",
      "|    value_loss         | 0.0146   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 109      |\n",
      "|    iterations         | 190      |\n",
      "|    time_elapsed       | 277      |\n",
      "|    total_timesteps    | 30400    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.93     |\n",
      "|    explained_variance | 0.418    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 189      |\n",
      "|    policy_loss        | -0.269   |\n",
      "|    std                | 0.055    |\n",
      "|    value_loss         | 0.0131   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=32000, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4        |\n",
      "|    mean_reward        | 0.635    |\n",
      "| time/                 |          |\n",
      "|    fps                | 107      |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 296      |\n",
      "|    total_timesteps    | 32000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.93     |\n",
      "|    explained_variance | 0.313    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | -0.0457  |\n",
      "|    std                | 0.055    |\n",
      "|    value_loss         | 0.0111   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 109      |\n",
      "|    iterations         | 210      |\n",
      "|    time_elapsed       | 307      |\n",
      "|    total_timesteps    | 33600    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.93     |\n",
      "|    explained_variance | 0.288    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 209      |\n",
      "|    policy_loss        | -0.145   |\n",
      "|    std                | 0.055    |\n",
      "|    value_loss         | 0.0135   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=35200, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "Eval num_timesteps=35200, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4        |\n",
      "|    mean_reward        | 0.632    |\n",
      "| time/                 |          |\n",
      "|    fps                | 108      |\n",
      "|    iterations         | 220      |\n",
      "|    time_elapsed       | 325      |\n",
      "|    total_timesteps    | 35200    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.93     |\n",
      "|    explained_variance | 0.427    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 219      |\n",
      "|    policy_loss        | -0.0441  |\n",
      "|    std                | 0.055    |\n",
      "|    value_loss         | 0.0103   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 109      |\n",
      "|    iterations         | 230      |\n",
      "|    time_elapsed       | 336      |\n",
      "|    total_timesteps    | 36800    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.93     |\n",
      "|    explained_variance | 0.549    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 229      |\n",
      "|    policy_loss        | -0.171   |\n",
      "|    std                | 0.055    |\n",
      "|    value_loss         | 0.0109   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=38400, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "Eval num_timesteps=38400, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4        |\n",
      "|    mean_reward        | 0.636    |\n",
      "| time/                 |          |\n",
      "|    fps                | 108      |\n",
      "|    iterations         | 240      |\n",
      "|    time_elapsed       | 355      |\n",
      "|    total_timesteps    | 38400    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.93     |\n",
      "|    explained_variance | 0.535    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 239      |\n",
      "|    policy_loss        | -0.00349 |\n",
      "|    std                | 0.055    |\n",
      "|    value_loss         | 0.0101   |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 109      |\n",
      "|    iterations         | 250      |\n",
      "|    time_elapsed       | 365      |\n",
      "|    total_timesteps    | 40000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.93     |\n",
      "|    explained_variance | 0.398    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 249      |\n",
      "|    policy_loss        | -0.235   |\n",
      "|    std                | 0.055    |\n",
      "|    value_loss         | 0.0142   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=41600, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "Eval num_timesteps=41600, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4        |\n",
      "|    mean_reward        | 0.631    |\n",
      "| time/                 |          |\n",
      "|    fps                | 108      |\n",
      "|    iterations         | 260      |\n",
      "|    time_elapsed       | 384      |\n",
      "|    total_timesteps    | 41600    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.93     |\n",
      "|    explained_variance | 0.541    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 259      |\n",
      "|    policy_loss        | -0.0427  |\n",
      "|    std                | 0.055    |\n",
      "|    value_loss         | 0.00917  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 109      |\n",
      "|    iterations         | 270      |\n",
      "|    time_elapsed       | 395      |\n",
      "|    total_timesteps    | 43200    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.93     |\n",
      "|    explained_variance | 0.629    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 269      |\n",
      "|    policy_loss        | -0.083   |\n",
      "|    std                | 0.055    |\n",
      "|    value_loss         | 0.0096   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=44800, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "Eval num_timesteps=44800, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4        |\n",
      "|    mean_reward        | 0.633    |\n",
      "| time/                 |          |\n",
      "|    fps                | 108      |\n",
      "|    iterations         | 280      |\n",
      "|    time_elapsed       | 414      |\n",
      "|    total_timesteps    | 44800    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.93     |\n",
      "|    explained_variance | 0.548    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 279      |\n",
      "|    policy_loss        | 0.066    |\n",
      "|    std                | 0.055    |\n",
      "|    value_loss         | 0.00971  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 109      |\n",
      "|    iterations         | 290      |\n",
      "|    time_elapsed       | 425      |\n",
      "|    total_timesteps    | 46400    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.93     |\n",
      "|    explained_variance | 0.682    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 289      |\n",
      "|    policy_loss        | -0.259   |\n",
      "|    std                | 0.055    |\n",
      "|    value_loss         | 0.0107   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=48000, episode_reward=0.61 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4        |\n",
      "|    mean_reward        | 0.633    |\n",
      "| time/                 |          |\n",
      "|    fps                | 108      |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 443      |\n",
      "|    total_timesteps    | 48000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.93     |\n",
      "|    explained_variance | 0.614    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | 0.0868   |\n",
      "|    std                | 0.055    |\n",
      "|    value_loss         | 0.00808  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 109      |\n",
      "|    iterations         | 310      |\n",
      "|    time_elapsed       | 454      |\n",
      "|    total_timesteps    | 49600    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.93     |\n",
      "|    explained_variance | 0.614    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 309      |\n",
      "|    policy_loss        | -0.142   |\n",
      "|    std                | 0.055    |\n",
      "|    value_loss         | 0.0104   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=51200, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "Eval num_timesteps=51200, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4        |\n",
      "|    mean_reward        | 0.634    |\n",
      "| time/                 |          |\n",
      "|    fps                | 108      |\n",
      "|    iterations         | 320      |\n",
      "|    time_elapsed       | 472      |\n",
      "|    total_timesteps    | 51200    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.93     |\n",
      "|    explained_variance | 0.578    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 319      |\n",
      "|    policy_loss        | 0.133    |\n",
      "|    std                | 0.055    |\n",
      "|    value_loss         | 0.00926  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 109      |\n",
      "|    iterations         | 330      |\n",
      "|    time_elapsed       | 483      |\n",
      "|    total_timesteps    | 52800    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.93     |\n",
      "|    explained_variance | 0.612    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 329      |\n",
      "|    policy_loss        | -0.142   |\n",
      "|    std                | 0.055    |\n",
      "|    value_loss         | 0.0113   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=54400, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "Eval num_timesteps=54400, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4        |\n",
      "|    mean_reward        | 0.635    |\n",
      "| time/                 |          |\n",
      "|    fps                | 108      |\n",
      "|    iterations         | 340      |\n",
      "|    time_elapsed       | 502      |\n",
      "|    total_timesteps    | 54400    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.93     |\n",
      "|    explained_variance | 0.584    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 339      |\n",
      "|    policy_loss        | 0.0112   |\n",
      "|    std                | 0.055    |\n",
      "|    value_loss         | 0.00942  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 109      |\n",
      "|    iterations         | 350      |\n",
      "|    time_elapsed       | 512      |\n",
      "|    total_timesteps    | 56000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.93     |\n",
      "|    explained_variance | 0.647    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 349      |\n",
      "|    policy_loss        | -0.186   |\n",
      "|    std                | 0.055    |\n",
      "|    value_loss         | 0.0109   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=57600, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "Eval num_timesteps=57600, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4        |\n",
      "|    mean_reward        | 0.634    |\n",
      "| time/                 |          |\n",
      "|    fps                | 108      |\n",
      "|    iterations         | 360      |\n",
      "|    time_elapsed       | 532      |\n",
      "|    total_timesteps    | 57600    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.93     |\n",
      "|    explained_variance | 0.657    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 359      |\n",
      "|    policy_loss        | 0.0331   |\n",
      "|    std                | 0.055    |\n",
      "|    value_loss         | 0.00757  |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 109      |\n",
      "|    iterations         | 370      |\n",
      "|    time_elapsed       | 542      |\n",
      "|    total_timesteps    | 59200    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.93     |\n",
      "|    explained_variance | 0.635    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 369      |\n",
      "|    policy_loss        | -0.11    |\n",
      "|    std                | 0.055    |\n",
      "|    value_loss         | 0.0109   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=60800, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60800, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4        |\n",
      "|    mean_reward        | 0.633    |\n",
      "| time/                 |          |\n",
      "|    fps                | 108      |\n",
      "|    iterations         | 380      |\n",
      "|    time_elapsed       | 561      |\n",
      "|    total_timesteps    | 60800    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.93     |\n",
      "|    explained_variance | 0.556    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 379      |\n",
      "|    policy_loss        | 0.0804   |\n",
      "|    std                | 0.055    |\n",
      "|    value_loss         | 0.0106   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 109      |\n",
      "|    iterations         | 390      |\n",
      "|    time_elapsed       | 572      |\n",
      "|    total_timesteps    | 62400    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.93     |\n",
      "|    explained_variance | 0.687    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 389      |\n",
      "|    policy_loss        | -0.167   |\n",
      "|    std                | 0.055    |\n",
      "|    value_loss         | 0.00991  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=64000, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=64000, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4        |\n",
      "|    mean_reward        | 0.634    |\n",
      "| time/                 |          |\n",
      "|    fps                | 108      |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 590      |\n",
      "|    total_timesteps    | 64000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.93     |\n",
      "|    explained_variance | 0.627    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | 0.0907   |\n",
      "|    std                | 0.055    |\n",
      "|    value_loss         | 0.00924  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 109      |\n",
      "|    iterations         | 410      |\n",
      "|    time_elapsed       | 601      |\n",
      "|    total_timesteps    | 65600    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.93     |\n",
      "|    explained_variance | 0.649    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 409      |\n",
      "|    policy_loss        | -0.0655  |\n",
      "|    std                | 0.055    |\n",
      "|    value_loss         | 0.00983  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=67200, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "Eval num_timesteps=67200, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4        |\n",
      "|    mean_reward        | 0.634    |\n",
      "| time/                 |          |\n",
      "|    fps                | 108      |\n",
      "|    iterations         | 420      |\n",
      "|    time_elapsed       | 620      |\n",
      "|    total_timesteps    | 67200    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.93     |\n",
      "|    explained_variance | 0.672    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 419      |\n",
      "|    policy_loss        | 0.131    |\n",
      "|    std                | 0.055    |\n",
      "|    value_loss         | 0.00874  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 109      |\n",
      "|    iterations         | 430      |\n",
      "|    time_elapsed       | 630      |\n",
      "|    total_timesteps    | 68800    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.93     |\n",
      "|    explained_variance | 0.602    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 429      |\n",
      "|    policy_loss        | -0.149   |\n",
      "|    std                | 0.0549   |\n",
      "|    value_loss         | 0.0121   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=70400, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "Eval num_timesteps=70400, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4        |\n",
      "|    mean_reward        | 0.631    |\n",
      "| time/                 |          |\n",
      "|    fps                | 108      |\n",
      "|    iterations         | 440      |\n",
      "|    time_elapsed       | 650      |\n",
      "|    total_timesteps    | 70400    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.93     |\n",
      "|    explained_variance | 0.686    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 439      |\n",
      "|    policy_loss        | 0.178    |\n",
      "|    std                | 0.055    |\n",
      "|    value_loss         | 0.00893  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 108      |\n",
      "|    iterations         | 450      |\n",
      "|    time_elapsed       | 660      |\n",
      "|    total_timesteps    | 72000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.93     |\n",
      "|    explained_variance | 0.638    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 449      |\n",
      "|    policy_loss        | -0.171   |\n",
      "|    std                | 0.0549   |\n",
      "|    value_loss         | 0.0115   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=73600, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "Eval num_timesteps=73600, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4        |\n",
      "|    mean_reward        | 0.634    |\n",
      "| time/                 |          |\n",
      "|    fps                | 108      |\n",
      "|    iterations         | 460      |\n",
      "|    time_elapsed       | 679      |\n",
      "|    total_timesteps    | 73600    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.93     |\n",
      "|    explained_variance | 0.625    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 459      |\n",
      "|    policy_loss        | 0.06     |\n",
      "|    std                | 0.0549   |\n",
      "|    value_loss         | 0.00948  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 108      |\n",
      "|    iterations         | 470      |\n",
      "|    time_elapsed       | 690      |\n",
      "|    total_timesteps    | 75200    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.93     |\n",
      "|    explained_variance | 0.725    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 469      |\n",
      "|    policy_loss        | -0.125   |\n",
      "|    std                | 0.0549   |\n",
      "|    value_loss         | 0.00861  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=76800, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "Eval num_timesteps=76800, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4        |\n",
      "|    mean_reward        | 0.633    |\n",
      "| time/                 |          |\n",
      "|    fps                | 108      |\n",
      "|    iterations         | 480      |\n",
      "|    time_elapsed       | 708      |\n",
      "|    total_timesteps    | 76800    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.93     |\n",
      "|    explained_variance | 0.678    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 479      |\n",
      "|    policy_loss        | 0.044    |\n",
      "|    std                | 0.0549   |\n",
      "|    value_loss         | 0.00905  |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 108      |\n",
      "|    iterations         | 490      |\n",
      "|    time_elapsed       | 719      |\n",
      "|    total_timesteps    | 78400    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.93     |\n",
      "|    explained_variance | 0.683    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 489      |\n",
      "|    policy_loss        | -0.23    |\n",
      "|    std                | 0.0549   |\n",
      "|    value_loss         | 0.0116   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4        |\n",
      "|    mean_reward        | 0.631    |\n",
      "| time/                 |          |\n",
      "|    fps                | 108      |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 738      |\n",
      "|    total_timesteps    | 80000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.93     |\n",
      "|    explained_variance | 0.666    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | 0.0933   |\n",
      "|    std                | 0.0549   |\n",
      "|    value_loss         | 0.00912  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 108      |\n",
      "|    iterations         | 510      |\n",
      "|    time_elapsed       | 749      |\n",
      "|    total_timesteps    | 81600    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.93     |\n",
      "|    explained_variance | 0.72     |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 509      |\n",
      "|    policy_loss        | -0.238   |\n",
      "|    std                | 0.0549   |\n",
      "|    value_loss         | 0.0105   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=83200, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=83200, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4        |\n",
      "|    mean_reward        | 0.634    |\n",
      "| time/                 |          |\n",
      "|    fps                | 108      |\n",
      "|    iterations         | 520      |\n",
      "|    time_elapsed       | 768      |\n",
      "|    total_timesteps    | 83200    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.93     |\n",
      "|    explained_variance | 0.623    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 519      |\n",
      "|    policy_loss        | 0.154    |\n",
      "|    std                | 0.0549   |\n",
      "|    value_loss         | 0.0101   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 108      |\n",
      "|    iterations         | 530      |\n",
      "|    time_elapsed       | 778      |\n",
      "|    total_timesteps    | 84800    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.93     |\n",
      "|    explained_variance | 0.654    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 529      |\n",
      "|    policy_loss        | -0.0849  |\n",
      "|    std                | 0.0549   |\n",
      "|    value_loss         | 0.0102   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=86400, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=86400, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4        |\n",
      "|    mean_reward        | 0.631    |\n",
      "| time/                 |          |\n",
      "|    fps                | 108      |\n",
      "|    iterations         | 540      |\n",
      "|    time_elapsed       | 797      |\n",
      "|    total_timesteps    | 86400    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.93     |\n",
      "|    explained_variance | 0.689    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 539      |\n",
      "|    policy_loss        | 0.107    |\n",
      "|    std                | 0.0549   |\n",
      "|    value_loss         | 0.00772  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 108      |\n",
      "|    iterations         | 550      |\n",
      "|    time_elapsed       | 808      |\n",
      "|    total_timesteps    | 88000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.93     |\n",
      "|    explained_variance | 0.662    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 549      |\n",
      "|    policy_loss        | -0.119   |\n",
      "|    std                | 0.0549   |\n",
      "|    value_loss         | 0.0111   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=89600, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "Eval num_timesteps=89600, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4        |\n",
      "|    mean_reward        | 0.632    |\n",
      "| time/                 |          |\n",
      "|    fps                | 108      |\n",
      "|    iterations         | 560      |\n",
      "|    time_elapsed       | 826      |\n",
      "|    total_timesteps    | 89600    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.93     |\n",
      "|    explained_variance | 0.694    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 559      |\n",
      "|    policy_loss        | 0.116    |\n",
      "|    std                | 0.0549   |\n",
      "|    value_loss         | 0.00803  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 109      |\n",
      "|    iterations         | 570      |\n",
      "|    time_elapsed       | 836      |\n",
      "|    total_timesteps    | 91200    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.93     |\n",
      "|    explained_variance | 0.667    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 569      |\n",
      "|    policy_loss        | -0.118   |\n",
      "|    std                | 0.0549   |\n",
      "|    value_loss         | 0.0105   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=92800, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "Eval num_timesteps=92800, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4        |\n",
      "|    mean_reward        | 0.634    |\n",
      "| time/                 |          |\n",
      "|    fps                | 108      |\n",
      "|    iterations         | 580      |\n",
      "|    time_elapsed       | 855      |\n",
      "|    total_timesteps    | 92800    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.93     |\n",
      "|    explained_variance | 0.618    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 579      |\n",
      "|    policy_loss        | 0.15     |\n",
      "|    std                | 0.0549   |\n",
      "|    value_loss         | 0.0104   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 109      |\n",
      "|    iterations         | 590      |\n",
      "|    time_elapsed       | 865      |\n",
      "|    total_timesteps    | 94400    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.93     |\n",
      "|    explained_variance | 0.663    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 589      |\n",
      "|    policy_loss        | -0.129   |\n",
      "|    std                | 0.0549   |\n",
      "|    value_loss         | 0.0102   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=96000, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "Eval num_timesteps=96000, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4        |\n",
      "|    mean_reward        | 0.632    |\n",
      "| time/                 |          |\n",
      "|    fps                | 108      |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 883      |\n",
      "|    total_timesteps    | 96000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.94     |\n",
      "|    explained_variance | 0.613    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | 0.183    |\n",
      "|    std                | 0.0549   |\n",
      "|    value_loss         | 0.00984  |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 109      |\n",
      "|    iterations         | 610      |\n",
      "|    time_elapsed       | 894      |\n",
      "|    total_timesteps    | 97600    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.94     |\n",
      "|    explained_variance | 0.664    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 609      |\n",
      "|    policy_loss        | -0.154   |\n",
      "|    std                | 0.0548   |\n",
      "|    value_loss         | 0.0109   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=99200, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "Eval num_timesteps=99200, episode_reward=0.64 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4        |\n",
      "|    mean_reward        | 0.635    |\n",
      "| time/                 |          |\n",
      "|    fps                | 108      |\n",
      "|    iterations         | 620      |\n",
      "|    time_elapsed       | 912      |\n",
      "|    total_timesteps    | 99200    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.94     |\n",
      "|    explained_variance | 0.624    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 619      |\n",
      "|    policy_loss        | 0.104    |\n",
      "|    std                | 0.0548   |\n",
      "|    value_loss         | 0.0108   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 109      |\n",
      "|    iterations         | 630      |\n",
      "|    time_elapsed       | 923      |\n",
      "|    total_timesteps    | 100800   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.94     |\n",
      "|    explained_variance | 0.659    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 629      |\n",
      "|    policy_loss        | -0.154   |\n",
      "|    std                | 0.0548   |\n",
      "|    value_loss         | 0.011    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=102400, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "Eval num_timesteps=102400, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4        |\n",
      "|    mean_reward        | 0.633    |\n",
      "| time/                 |          |\n",
      "|    fps                | 108      |\n",
      "|    iterations         | 640      |\n",
      "|    time_elapsed       | 942      |\n",
      "|    total_timesteps    | 102400   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.94     |\n",
      "|    explained_variance | 0.614    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 639      |\n",
      "|    policy_loss        | 0.159    |\n",
      "|    std                | 0.0548   |\n",
      "|    value_loss         | 0.0106   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 109      |\n",
      "|    iterations         | 650      |\n",
      "|    time_elapsed       | 952      |\n",
      "|    total_timesteps    | 104000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.94     |\n",
      "|    explained_variance | 0.666    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 649      |\n",
      "|    policy_loss        | -0.119   |\n",
      "|    std                | 0.0548   |\n",
      "|    value_loss         | 0.0113   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=105600, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "Eval num_timesteps=105600, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4        |\n",
      "|    mean_reward        | 0.633    |\n",
      "| time/                 |          |\n",
      "|    fps                | 108      |\n",
      "|    iterations         | 660      |\n",
      "|    time_elapsed       | 971      |\n",
      "|    total_timesteps    | 105600   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.94     |\n",
      "|    explained_variance | 0.663    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 659      |\n",
      "|    policy_loss        | 0.126    |\n",
      "|    std                | 0.0548   |\n",
      "|    value_loss         | 0.00953  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 109      |\n",
      "|    iterations         | 670      |\n",
      "|    time_elapsed       | 981      |\n",
      "|    total_timesteps    | 107200   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.94     |\n",
      "|    explained_variance | 0.645    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 669      |\n",
      "|    policy_loss        | -0.151   |\n",
      "|    std                | 0.0548   |\n",
      "|    value_loss         | 0.0118   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=108800, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "Eval num_timesteps=108800, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4        |\n",
      "|    mean_reward        | 0.633    |\n",
      "| time/                 |          |\n",
      "|    fps                | 108      |\n",
      "|    iterations         | 680      |\n",
      "|    time_elapsed       | 1000     |\n",
      "|    total_timesteps    | 108800   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.94     |\n",
      "|    explained_variance | 0.647    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 679      |\n",
      "|    policy_loss        | 0.159    |\n",
      "|    std                | 0.0548   |\n",
      "|    value_loss         | 0.00844  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 109      |\n",
      "|    iterations         | 690      |\n",
      "|    time_elapsed       | 1010     |\n",
      "|    total_timesteps    | 110400   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.94     |\n",
      "|    explained_variance | 0.65     |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 689      |\n",
      "|    policy_loss        | -0.0751  |\n",
      "|    std                | 0.0548   |\n",
      "|    value_loss         | 0.00993  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=112000, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "Eval num_timesteps=112000, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4        |\n",
      "|    mean_reward        | 0.631    |\n",
      "| time/                 |          |\n",
      "|    fps                | 108      |\n",
      "|    iterations         | 700      |\n",
      "|    time_elapsed       | 1029     |\n",
      "|    total_timesteps    | 112000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.94     |\n",
      "|    explained_variance | 0.686    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | 0.0883   |\n",
      "|    std                | 0.0548   |\n",
      "|    value_loss         | 0.00861  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 109      |\n",
      "|    iterations         | 710      |\n",
      "|    time_elapsed       | 1039     |\n",
      "|    total_timesteps    | 113600   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.94     |\n",
      "|    explained_variance | 0.725    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 709      |\n",
      "|    policy_loss        | -0.211   |\n",
      "|    std                | 0.0548   |\n",
      "|    value_loss         | 0.0106   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=115200, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "Eval num_timesteps=115200, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4        |\n",
      "|    mean_reward        | 0.632    |\n",
      "| time/                 |          |\n",
      "|    fps                | 108      |\n",
      "|    iterations         | 720      |\n",
      "|    time_elapsed       | 1058     |\n",
      "|    total_timesteps    | 115200   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.94     |\n",
      "|    explained_variance | 0.636    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 719      |\n",
      "|    policy_loss        | 0.257    |\n",
      "|    std                | 0.0548   |\n",
      "|    value_loss         | 0.0126   |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 109      |\n",
      "|    iterations         | 730      |\n",
      "|    time_elapsed       | 1069     |\n",
      "|    total_timesteps    | 116800   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.94     |\n",
      "|    explained_variance | 0.542    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 729      |\n",
      "|    policy_loss        | -0.164   |\n",
      "|    std                | 0.0548   |\n",
      "|    value_loss         | 0.0146   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=118400, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "Eval num_timesteps=118400, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4        |\n",
      "|    mean_reward        | 0.63     |\n",
      "| time/                 |          |\n",
      "|    fps                | 108      |\n",
      "|    iterations         | 740      |\n",
      "|    time_elapsed       | 1087     |\n",
      "|    total_timesteps    | 118400   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.94     |\n",
      "|    explained_variance | 0.672    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 739      |\n",
      "|    policy_loss        | 0.233    |\n",
      "|    std                | 0.0548   |\n",
      "|    value_loss         | 0.0102   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 109      |\n",
      "|    iterations         | 750      |\n",
      "|    time_elapsed       | 1097     |\n",
      "|    total_timesteps    | 120000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.94     |\n",
      "|    explained_variance | 0.65     |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 749      |\n",
      "|    policy_loss        | -0.144   |\n",
      "|    std                | 0.0548   |\n",
      "|    value_loss         | 0.0116   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=121600, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "Eval num_timesteps=121600, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4        |\n",
      "|    mean_reward        | 0.633    |\n",
      "| time/                 |          |\n",
      "|    fps                | 108      |\n",
      "|    iterations         | 760      |\n",
      "|    time_elapsed       | 1116     |\n",
      "|    total_timesteps    | 121600   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.94     |\n",
      "|    explained_variance | 0.586    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 759      |\n",
      "|    policy_loss        | 0.159    |\n",
      "|    std                | 0.0548   |\n",
      "|    value_loss         | 0.0119   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 109      |\n",
      "|    iterations         | 770      |\n",
      "|    time_elapsed       | 1126     |\n",
      "|    total_timesteps    | 123200   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.94     |\n",
      "|    explained_variance | 0.626    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 769      |\n",
      "|    policy_loss        | -0.161   |\n",
      "|    std                | 0.0548   |\n",
      "|    value_loss         | 0.0122   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=124800, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "Eval num_timesteps=124800, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4        |\n",
      "|    mean_reward        | 0.63     |\n",
      "| time/                 |          |\n",
      "|    fps                | 109      |\n",
      "|    iterations         | 780      |\n",
      "|    time_elapsed       | 1144     |\n",
      "|    total_timesteps    | 124800   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.94     |\n",
      "|    explained_variance | 0.657    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 779      |\n",
      "|    policy_loss        | 0.236    |\n",
      "|    std                | 0.0548   |\n",
      "|    value_loss         | 0.0106   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 109      |\n",
      "|    iterations         | 790      |\n",
      "|    time_elapsed       | 1155     |\n",
      "|    total_timesteps    | 126400   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.94     |\n",
      "|    explained_variance | 0.645    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 789      |\n",
      "|    policy_loss        | -0.105   |\n",
      "|    std                | 0.0548   |\n",
      "|    value_loss         | 0.0111   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=128000, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "Eval num_timesteps=128000, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4        |\n",
      "|    mean_reward        | 0.634    |\n",
      "| time/                 |          |\n",
      "|    fps                | 109      |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 1173     |\n",
      "|    total_timesteps    | 128000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.94     |\n",
      "|    explained_variance | 0.656    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | 0.23     |\n",
      "|    std                | 0.0548   |\n",
      "|    value_loss         | 0.00996  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 109      |\n",
      "|    iterations         | 810      |\n",
      "|    time_elapsed       | 1184     |\n",
      "|    total_timesteps    | 129600   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.94     |\n",
      "|    explained_variance | 0.691    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 809      |\n",
      "|    policy_loss        | -0.141   |\n",
      "|    std                | 0.0548   |\n",
      "|    value_loss         | 0.0111   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=131200, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "Eval num_timesteps=131200, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4        |\n",
      "|    mean_reward        | 0.633    |\n",
      "| time/                 |          |\n",
      "|    fps                | 109      |\n",
      "|    iterations         | 820      |\n",
      "|    time_elapsed       | 1203     |\n",
      "|    total_timesteps    | 131200   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.94     |\n",
      "|    explained_variance | 0.645    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 819      |\n",
      "|    policy_loss        | 0.17     |\n",
      "|    std                | 0.0547   |\n",
      "|    value_loss         | 0.0103   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 109      |\n",
      "|    iterations         | 830      |\n",
      "|    time_elapsed       | 1213     |\n",
      "|    total_timesteps    | 132800   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.95     |\n",
      "|    explained_variance | 0.579    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 829      |\n",
      "|    policy_loss        | -0.0863  |\n",
      "|    std                | 0.0547   |\n",
      "|    value_loss         | 0.0131   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=134400, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "Eval num_timesteps=134400, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4        |\n",
      "|    mean_reward        | 0.633    |\n",
      "| time/                 |          |\n",
      "|    fps                | 109      |\n",
      "|    iterations         | 840      |\n",
      "|    time_elapsed       | 1232     |\n",
      "|    total_timesteps    | 134400   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.95     |\n",
      "|    explained_variance | 0.678    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 839      |\n",
      "|    policy_loss        | 0.142    |\n",
      "|    std                | 0.0547   |\n",
      "|    value_loss         | 0.00928  |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 109      |\n",
      "|    iterations         | 850      |\n",
      "|    time_elapsed       | 1242     |\n",
      "|    total_timesteps    | 136000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.95     |\n",
      "|    explained_variance | 0.719    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 849      |\n",
      "|    policy_loss        | -0.136   |\n",
      "|    std                | 0.0547   |\n",
      "|    value_loss         | 0.00984  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=137600, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "Eval num_timesteps=137600, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4        |\n",
      "|    mean_reward        | 0.632    |\n",
      "| time/                 |          |\n",
      "|    fps                | 109      |\n",
      "|    iterations         | 860      |\n",
      "|    time_elapsed       | 1260     |\n",
      "|    total_timesteps    | 137600   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.95     |\n",
      "|    explained_variance | 0.575    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 859      |\n",
      "|    policy_loss        | 0.0759   |\n",
      "|    std                | 0.0547   |\n",
      "|    value_loss         | 0.00999  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 109      |\n",
      "|    iterations         | 870      |\n",
      "|    time_elapsed       | 1271     |\n",
      "|    total_timesteps    | 139200   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.95     |\n",
      "|    explained_variance | 0.667    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 869      |\n",
      "|    policy_loss        | -0.197   |\n",
      "|    std                | 0.0547   |\n",
      "|    value_loss         | 0.0116   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=140800, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "Eval num_timesteps=140800, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4        |\n",
      "|    mean_reward        | 0.63     |\n",
      "| time/                 |          |\n",
      "|    fps                | 109      |\n",
      "|    iterations         | 880      |\n",
      "|    time_elapsed       | 1289     |\n",
      "|    total_timesteps    | 140800   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.95     |\n",
      "|    explained_variance | 0.662    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 879      |\n",
      "|    policy_loss        | 0.0363   |\n",
      "|    std                | 0.0547   |\n",
      "|    value_loss         | 0.01     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 109      |\n",
      "|    iterations         | 890      |\n",
      "|    time_elapsed       | 1300     |\n",
      "|    total_timesteps    | 142400   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.95     |\n",
      "|    explained_variance | 0.628    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 889      |\n",
      "|    policy_loss        | -0.177   |\n",
      "|    std                | 0.0547   |\n",
      "|    value_loss         | 0.013    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=144000, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "Eval num_timesteps=144000, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4        |\n",
      "|    mean_reward        | 0.632    |\n",
      "| time/                 |          |\n",
      "|    fps                | 109      |\n",
      "|    iterations         | 900      |\n",
      "|    time_elapsed       | 1319     |\n",
      "|    total_timesteps    | 144000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.95     |\n",
      "|    explained_variance | 0.59     |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 899      |\n",
      "|    policy_loss        | 0.192    |\n",
      "|    std                | 0.0547   |\n",
      "|    value_loss         | 0.0115   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 109      |\n",
      "|    iterations         | 910      |\n",
      "|    time_elapsed       | 1329     |\n",
      "|    total_timesteps    | 145600   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.95     |\n",
      "|    explained_variance | 0.719    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 909      |\n",
      "|    policy_loss        | -0.198   |\n",
      "|    std                | 0.0547   |\n",
      "|    value_loss         | 0.0101   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=147200, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "Eval num_timesteps=147200, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4        |\n",
      "|    mean_reward        | 0.632    |\n",
      "| time/                 |          |\n",
      "|    fps                | 109      |\n",
      "|    iterations         | 920      |\n",
      "|    time_elapsed       | 1347     |\n",
      "|    total_timesteps    | 147200   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.95     |\n",
      "|    explained_variance | 0.69     |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 919      |\n",
      "|    policy_loss        | 0.0457   |\n",
      "|    std                | 0.0547   |\n",
      "|    value_loss         | 0.0088   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 109      |\n",
      "|    iterations         | 930      |\n",
      "|    time_elapsed       | 1358     |\n",
      "|    total_timesteps    | 148800   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.95     |\n",
      "|    explained_variance | 0.632    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 929      |\n",
      "|    policy_loss        | -0.115   |\n",
      "|    std                | 0.0547   |\n",
      "|    value_loss         | 0.011    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=150400, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "Eval num_timesteps=150400, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4        |\n",
      "|    mean_reward        | 0.633    |\n",
      "| time/                 |          |\n",
      "|    fps                | 109      |\n",
      "|    iterations         | 940      |\n",
      "|    time_elapsed       | 1376     |\n",
      "|    total_timesteps    | 150400   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.95     |\n",
      "|    explained_variance | 0.607    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 939      |\n",
      "|    policy_loss        | 0.191    |\n",
      "|    std                | 0.0547   |\n",
      "|    value_loss         | 0.0112   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 109      |\n",
      "|    iterations         | 950      |\n",
      "|    time_elapsed       | 1386     |\n",
      "|    total_timesteps    | 152000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.95     |\n",
      "|    explained_variance | 0.598    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 949      |\n",
      "|    policy_loss        | -0.126   |\n",
      "|    std                | 0.0547   |\n",
      "|    value_loss         | 0.0122   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=153600, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "Eval num_timesteps=153600, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4        |\n",
      "|    mean_reward        | 0.633    |\n",
      "| time/                 |          |\n",
      "|    fps                | 109      |\n",
      "|    iterations         | 960      |\n",
      "|    time_elapsed       | 1404     |\n",
      "|    total_timesteps    | 153600   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.95     |\n",
      "|    explained_variance | 0.696    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 959      |\n",
      "|    policy_loss        | 0.129    |\n",
      "|    std                | 0.0547   |\n",
      "|    value_loss         | 0.00901  |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 109      |\n",
      "|    iterations         | 970      |\n",
      "|    time_elapsed       | 1415     |\n",
      "|    total_timesteps    | 155200   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.95     |\n",
      "|    explained_variance | 0.59     |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 969      |\n",
      "|    policy_loss        | -0.168   |\n",
      "|    std                | 0.0546   |\n",
      "|    value_loss         | 0.0134   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=156800, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "Eval num_timesteps=156800, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4        |\n",
      "|    mean_reward        | 0.632    |\n",
      "| time/                 |          |\n",
      "|    fps                | 109      |\n",
      "|    iterations         | 980      |\n",
      "|    time_elapsed       | 1433     |\n",
      "|    total_timesteps    | 156800   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.95     |\n",
      "|    explained_variance | 0.654    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 979      |\n",
      "|    policy_loss        | 0.15     |\n",
      "|    std                | 0.0546   |\n",
      "|    value_loss         | 0.00993  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 109      |\n",
      "|    iterations         | 990      |\n",
      "|    time_elapsed       | 1444     |\n",
      "|    total_timesteps    | 158400   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.95     |\n",
      "|    explained_variance | 0.571    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 989      |\n",
      "|    policy_loss        | -0.112   |\n",
      "|    std                | 0.0546   |\n",
      "|    value_loss         | 0.0121   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=160000, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4        |\n",
      "|    mean_reward        | 0.633    |\n",
      "| time/                 |          |\n",
      "|    fps                | 109      |\n",
      "|    iterations         | 1000     |\n",
      "|    time_elapsed       | 1462     |\n",
      "|    total_timesteps    | 160000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.95     |\n",
      "|    explained_variance | 0.695    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | 0.0592   |\n",
      "|    std                | 0.0546   |\n",
      "|    value_loss         | 0.00887  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 109      |\n",
      "|    iterations         | 1010     |\n",
      "|    time_elapsed       | 1473     |\n",
      "|    total_timesteps    | 161600   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.95     |\n",
      "|    explained_variance | 0.665    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 1009     |\n",
      "|    policy_loss        | -0.16    |\n",
      "|    std                | 0.0546   |\n",
      "|    value_loss         | 0.0116   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=163200, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "Eval num_timesteps=163200, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4        |\n",
      "|    mean_reward        | 0.633    |\n",
      "| time/                 |          |\n",
      "|    fps                | 109      |\n",
      "|    iterations         | 1020     |\n",
      "|    time_elapsed       | 1491     |\n",
      "|    total_timesteps    | 163200   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.96     |\n",
      "|    explained_variance | 0.656    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 1019     |\n",
      "|    policy_loss        | 0.16     |\n",
      "|    std                | 0.0546   |\n",
      "|    value_loss         | 0.00966  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 109      |\n",
      "|    iterations         | 1030     |\n",
      "|    time_elapsed       | 1501     |\n",
      "|    total_timesteps    | 164800   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.96     |\n",
      "|    explained_variance | 0.663    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 1029     |\n",
      "|    policy_loss        | -0.181   |\n",
      "|    std                | 0.0546   |\n",
      "|    value_loss         | 0.0124   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=166400, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "Eval num_timesteps=166400, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4        |\n",
      "|    mean_reward        | 0.632    |\n",
      "| time/                 |          |\n",
      "|    fps                | 109      |\n",
      "|    iterations         | 1040     |\n",
      "|    time_elapsed       | 1520     |\n",
      "|    total_timesteps    | 166400   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.96     |\n",
      "|    explained_variance | 0.62     |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 1039     |\n",
      "|    policy_loss        | 0.0624   |\n",
      "|    std                | 0.0546   |\n",
      "|    value_loss         | 0.0112   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 109      |\n",
      "|    iterations         | 1050     |\n",
      "|    time_elapsed       | 1530     |\n",
      "|    total_timesteps    | 168000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.96     |\n",
      "|    explained_variance | 0.739    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 1049     |\n",
      "|    policy_loss        | -0.219   |\n",
      "|    std                | 0.0546   |\n",
      "|    value_loss         | 0.0102   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=169600, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "Eval num_timesteps=169600, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4        |\n",
      "|    mean_reward        | 0.633    |\n",
      "| time/                 |          |\n",
      "|    fps                | 109      |\n",
      "|    iterations         | 1060     |\n",
      "|    time_elapsed       | 1549     |\n",
      "|    total_timesteps    | 169600   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.96     |\n",
      "|    explained_variance | 0.641    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 1059     |\n",
      "|    policy_loss        | 0.208    |\n",
      "|    std                | 0.0546   |\n",
      "|    value_loss         | 0.0108   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 109      |\n",
      "|    iterations         | 1070     |\n",
      "|    time_elapsed       | 1559     |\n",
      "|    total_timesteps    | 171200   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.96     |\n",
      "|    explained_variance | 0.675    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 1069     |\n",
      "|    policy_loss        | -0.243   |\n",
      "|    std                | 0.0546   |\n",
      "|    value_loss         | 0.0125   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=172800, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "Eval num_timesteps=172800, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4        |\n",
      "|    mean_reward        | 0.634    |\n",
      "| time/                 |          |\n",
      "|    fps                | 109      |\n",
      "|    iterations         | 1080     |\n",
      "|    time_elapsed       | 1578     |\n",
      "|    total_timesteps    | 172800   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.96     |\n",
      "|    explained_variance | 0.63     |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 1079     |\n",
      "|    policy_loss        | 0.206    |\n",
      "|    std                | 0.0545   |\n",
      "|    value_loss         | 0.0106   |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 109      |\n",
      "|    iterations         | 1090     |\n",
      "|    time_elapsed       | 1588     |\n",
      "|    total_timesteps    | 174400   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.96     |\n",
      "|    explained_variance | 0.623    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 1089     |\n",
      "|    policy_loss        | -0.107   |\n",
      "|    std                | 0.0545   |\n",
      "|    value_loss         | 0.0121   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=176000, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "Eval num_timesteps=176000, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4        |\n",
      "|    mean_reward        | 0.634    |\n",
      "| time/                 |          |\n",
      "|    fps                | 109      |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 1606     |\n",
      "|    total_timesteps    | 176000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.96     |\n",
      "|    explained_variance | 0.657    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | 0.277    |\n",
      "|    std                | 0.0545   |\n",
      "|    value_loss         | 0.0116   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 109      |\n",
      "|    iterations         | 1110     |\n",
      "|    time_elapsed       | 1617     |\n",
      "|    total_timesteps    | 177600   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.96     |\n",
      "|    explained_variance | 0.635    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 1109     |\n",
      "|    policy_loss        | -0.157   |\n",
      "|    std                | 0.0545   |\n",
      "|    value_loss         | 0.0118   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=179200, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "Eval num_timesteps=179200, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4        |\n",
      "|    mean_reward        | 0.633    |\n",
      "| time/                 |          |\n",
      "|    fps                | 109      |\n",
      "|    iterations         | 1120     |\n",
      "|    time_elapsed       | 1635     |\n",
      "|    total_timesteps    | 179200   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.96     |\n",
      "|    explained_variance | 0.645    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 1119     |\n",
      "|    policy_loss        | 0.139    |\n",
      "|    std                | 0.0545   |\n",
      "|    value_loss         | 0.0104   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 109      |\n",
      "|    iterations         | 1130     |\n",
      "|    time_elapsed       | 1646     |\n",
      "|    total_timesteps    | 180800   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.96     |\n",
      "|    explained_variance | 0.59     |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 1129     |\n",
      "|    policy_loss        | -0.143   |\n",
      "|    std                | 0.0545   |\n",
      "|    value_loss         | 0.0125   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=182400, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "Eval num_timesteps=182400, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4        |\n",
      "|    mean_reward        | 0.632    |\n",
      "| time/                 |          |\n",
      "|    fps                | 109      |\n",
      "|    iterations         | 1140     |\n",
      "|    time_elapsed       | 1664     |\n",
      "|    total_timesteps    | 182400   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.96     |\n",
      "|    explained_variance | 0.652    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 1139     |\n",
      "|    policy_loss        | 0.0645   |\n",
      "|    std                | 0.0545   |\n",
      "|    value_loss         | 0.0101   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 109      |\n",
      "|    iterations         | 1150     |\n",
      "|    time_elapsed       | 1674     |\n",
      "|    total_timesteps    | 184000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.96     |\n",
      "|    explained_variance | 0.622    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 1149     |\n",
      "|    policy_loss        | -0.161   |\n",
      "|    std                | 0.0545   |\n",
      "|    value_loss         | 0.0128   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=185600, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "Eval num_timesteps=185600, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4        |\n",
      "|    mean_reward        | 0.631    |\n",
      "| time/                 |          |\n",
      "|    fps                | 109      |\n",
      "|    iterations         | 1160     |\n",
      "|    time_elapsed       | 1693     |\n",
      "|    total_timesteps    | 185600   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.96     |\n",
      "|    explained_variance | 0.686    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 1159     |\n",
      "|    policy_loss        | 0.131    |\n",
      "|    std                | 0.0545   |\n",
      "|    value_loss         | 0.00885  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 109      |\n",
      "|    iterations         | 1170     |\n",
      "|    time_elapsed       | 1703     |\n",
      "|    total_timesteps    | 187200   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.96     |\n",
      "|    explained_variance | 0.7      |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 1169     |\n",
      "|    policy_loss        | -0.141   |\n",
      "|    std                | 0.0545   |\n",
      "|    value_loss         | 0.0108   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=188800, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "Eval num_timesteps=188800, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4        |\n",
      "|    mean_reward        | 0.633    |\n",
      "| time/                 |          |\n",
      "|    fps                | 109      |\n",
      "|    iterations         | 1180     |\n",
      "|    time_elapsed       | 1722     |\n",
      "|    total_timesteps    | 188800   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.96     |\n",
      "|    explained_variance | 0.661    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 1179     |\n",
      "|    policy_loss        | 0.179    |\n",
      "|    std                | 0.0545   |\n",
      "|    value_loss         | 0.0101   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 109      |\n",
      "|    iterations         | 1190     |\n",
      "|    time_elapsed       | 1732     |\n",
      "|    total_timesteps    | 190400   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.96     |\n",
      "|    explained_variance | 0.738    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 1189     |\n",
      "|    policy_loss        | -0.183   |\n",
      "|    std                | 0.0545   |\n",
      "|    value_loss         | 0.0103   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=192000, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "Eval num_timesteps=192000, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4        |\n",
      "|    mean_reward        | 0.631    |\n",
      "| time/                 |          |\n",
      "|    fps                | 109      |\n",
      "|    iterations         | 1200     |\n",
      "|    time_elapsed       | 1750     |\n",
      "|    total_timesteps    | 192000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.96     |\n",
      "|    explained_variance | 0.691    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 1199     |\n",
      "|    policy_loss        | 0.153    |\n",
      "|    std                | 0.0545   |\n",
      "|    value_loss         | 0.00955  |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 109      |\n",
      "|    iterations         | 1210     |\n",
      "|    time_elapsed       | 1760     |\n",
      "|    total_timesteps    | 193600   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.96     |\n",
      "|    explained_variance | 0.652    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 1209     |\n",
      "|    policy_loss        | -0.166   |\n",
      "|    std                | 0.0545   |\n",
      "|    value_loss         | 0.012    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=195200, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "Eval num_timesteps=195200, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4        |\n",
      "|    mean_reward        | 0.632    |\n",
      "| time/                 |          |\n",
      "|    fps                | 109      |\n",
      "|    iterations         | 1220     |\n",
      "|    time_elapsed       | 1779     |\n",
      "|    total_timesteps    | 195200   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.96     |\n",
      "|    explained_variance | 0.707    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 1219     |\n",
      "|    policy_loss        | 0.156    |\n",
      "|    std                | 0.0545   |\n",
      "|    value_loss         | 0.00907  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 109      |\n",
      "|    iterations         | 1230     |\n",
      "|    time_elapsed       | 1790     |\n",
      "|    total_timesteps    | 196800   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.96     |\n",
      "|    explained_variance | 0.64     |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 1229     |\n",
      "|    policy_loss        | -0.139   |\n",
      "|    std                | 0.0545   |\n",
      "|    value_loss         | 0.0122   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=198400, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "Eval num_timesteps=198400, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4        |\n",
      "|    mean_reward        | 0.633    |\n",
      "| time/                 |          |\n",
      "|    fps                | 109      |\n",
      "|    iterations         | 1240     |\n",
      "|    time_elapsed       | 1808     |\n",
      "|    total_timesteps    | 198400   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.96     |\n",
      "|    explained_variance | 0.628    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 1239     |\n",
      "|    policy_loss        | 0.15     |\n",
      "|    std                | 0.0545   |\n",
      "|    value_loss         | 0.0118   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 109      |\n",
      "|    iterations         | 1250     |\n",
      "|    time_elapsed       | 1819     |\n",
      "|    total_timesteps    | 200000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.96     |\n",
      "|    explained_variance | 0.641    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 1249     |\n",
      "|    policy_loss        | -0.101   |\n",
      "|    std                | 0.0545   |\n",
      "|    value_loss         | 0.0119   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=201600, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "Eval num_timesteps=201600, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4        |\n",
      "|    mean_reward        | 0.63     |\n",
      "| time/                 |          |\n",
      "|    fps                | 109      |\n",
      "|    iterations         | 1260     |\n",
      "|    time_elapsed       | 1838     |\n",
      "|    total_timesteps    | 201600   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.96     |\n",
      "|    explained_variance | 0.648    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 1259     |\n",
      "|    policy_loss        | 0.17     |\n",
      "|    std                | 0.0545   |\n",
      "|    value_loss         | 0.0105   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 109      |\n",
      "|    iterations         | 1270     |\n",
      "|    time_elapsed       | 1848     |\n",
      "|    total_timesteps    | 203200   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.97     |\n",
      "|    explained_variance | 0.581    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 1269     |\n",
      "|    policy_loss        | -0.102   |\n",
      "|    std                | 0.0545   |\n",
      "|    value_loss         | 0.0136   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=204800, episode_reward=0.62 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "Eval num_timesteps=204800, episode_reward=0.63 +/- 0.00\n",
      "Episode length: 4.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 4        |\n",
      "|    mean_reward        | 0.632    |\n",
      "| time/                 |          |\n",
      "|    fps                | 109      |\n",
      "|    iterations         | 1280     |\n",
      "|    time_elapsed       | 1867     |\n",
      "|    total_timesteps    | 204800   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.97     |\n",
      "|    explained_variance | 0.677    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 1279     |\n",
      "|    policy_loss        | 0.144    |\n",
      "|    std                | 0.0544   |\n",
      "|    value_loss         | 0.00937  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 109      |\n",
      "|    iterations         | 1290     |\n",
      "|    time_elapsed       | 1877     |\n",
      "|    total_timesteps    | 206400   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | 5.97     |\n",
      "|    explained_variance | 0.68     |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 1289     |\n",
      "|    policy_loss        | -0.115   |\n",
      "|    std                | 0.0544   |\n",
      "|    value_loss         | 0.0107   |\n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for seed in range(1,4):\n",
    "    if True:\n",
    "        print(f'seed {seed}')\n",
    "        log_dir = './data/'+case+'/seed_'+str(seed)\n",
    "        os.makedirs(log_dir, exist_ok=True)\n",
    "        num_cpu = 32\n",
    "        env_train.seed(seed)\n",
    "        env_eval.seed(seed)\n",
    "        env_train_ = env_wrappers(env_train, x_coords, y_coords)\n",
    "        env_eval_ = env_wrappers(env_eval, x_coords, y_coords)\n",
    "        train_callback = CustomEvalCallback(env_train_, \n",
    "                                            best_model_save_path=None, \n",
    "                                            n_eval_episodes=1,\n",
    "                                            log_path=str(log_dir)+'/results_train', \n",
    "                                            eval_freq=100)\n",
    "        callback_list = [train_callback]\n",
    "        eval_callback = CustomEvalCallback(env_eval_, \n",
    "                                           best_model_save_path=str(log_dir)+'/best_model_eval', \n",
    "                                           n_eval_episodes=1,\n",
    "                                           log_path=str(log_dir)+'/results_eval', \n",
    "                                           eval_freq=100)\n",
    "        callback_list.append(eval_callback)\n",
    "        callback = CallbackList(callback_list)\n",
    "        env = SubprocVecEnv([make_env(env_train_, i, seed) for i in range(num_cpu)])\n",
    "        print(env.observation_space)\n",
    "#     env = VecMonitor(env, filename=log_dir)\n",
    "        print(f'seed {seed}: model definition ..')\n",
    "        model = A2C(policy=MlpPolicy,\n",
    "                    env=env,\n",
    "                    learning_rate = 1e-4,\n",
    "                    n_steps = 5,\n",
    "                    gamma = 0.99,\n",
    "                    gae_lambda = 0.95,\n",
    "                    ent_coef = 0.001,\n",
    "                    vf_coef = 0.5,\n",
    "                    max_grad_norm = 0.5,\n",
    "                    use_sde= False,\n",
    "                    create_eval_env= False,\n",
    "                    policy_kwargs = dict(net_arch=[20,20], \n",
    "                                         log_std_init=-2.9),\n",
    "                    verbose = 1,\n",
    "                    seed = seed,\n",
    "                    device = \"auto\")\n",
    "#         model_params = model_pretrained.get_parameters()\n",
    "#         model.set_parameters(model_params, exact_match=False)\n",
    "        model.get_parameters()['policy']['mlp_extractor.shared_net.0.weight'][:,4:] = 0\n",
    "        print(f'seed {seed}: learning ..')\n",
    "        model.learn(total_timesteps=300000, callback=callback, log_interval=10)\n",
    "        model.save(log_dir+'/A2C')\n",
    "        fig = plot_learning(log_dir, case='train')\n",
    "        fig.savefig(log_dir+'/learn_train.png')\n",
    "        fig = plot_learning(log_dir, case='eval')\n",
    "        fig.savefig(log_dir+'/learn_eval.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
